
import numpy as np, copy
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from scipy import stats

def train_cnn(cnn_model, data_chirp_train, GT_train, data_chirp_val, GT_val, batch_size, noEpochs, savingTime,
              save_path_loss):
    lowestInnLoss = 800
    countSave = 0
    # Saving results parameters
    Results_train = np.zeros((int(np.floor(noEpochs / savingTime)), 5))  #
    loss_train = np.zeros((int(np.floor(noEpochs / savingTime))))
    Results_val = np.zeros((int(np.floor(noEpochs / savingTime)), 5))  #
    loss_val = np.zeros((int(np.floor(noEpochs / savingTime))))
    for epochI in range(noEpochs):
        # with each epoch reuse the original training data that is not modified
        # Without copy.deepcopy to make a copy of the list, both lists point to the same data.
        data_chirp_train_c = copy.deepcopy(data_chirp_train)

        history = cnn_model.fit(data_chirp_train_c,
                                GT_train,
                                batch_size=batch_size,
                                epochs=1,
                                verbose=1)
        if (epochI % savingTime == 0):
            # Results on training data without using mini-batches
            predictionsPerRound = cnn_model.predict(data_chirp_train_c)[:, 0]

            loss_train[countSave] = mean_squared_error(GT_train,
                                                           predictionsPerRound)  # loss or history.history['loss']
            Results_train[countSave, 0] = np.sqrt(mean_squared_error(GT_train, predictionsPerRound))
            Results_train[countSave, 1] = mean_absolute_error(GT_train, predictionsPerRound)
            Results_train[countSave, 2] = r2_score(GT_train, predictionsPerRound)
            corrI = stats.pearsonr(GT_train, predictionsPerRound)
            Results_train[countSave, 3] = corrI[0]
            Results_train[countSave, 4] = corrI[1]

            # Results on validation data
            predictionsPerRound = cnn_model.predict(data_chirp_val)[:, 0]

            loss_val[countSave] = mean_squared_error(GT_val, predictionsPerRound)  # loss
            Results_val[countSave, 0] = np.sqrt(mean_squared_error(GT_val, predictionsPerRound))
            Results_val[countSave, 1] = mean_absolute_error(GT_val, predictionsPerRound)
            Results_val[countSave, 2] = r2_score(GT_val, predictionsPerRound)
            corrI = stats.pearsonr(GT_val, predictionsPerRound)
            Results_val[countSave, 3] = corrI[0]
            Results_val[countSave, 4] = corrI[1]
            print("Epoch # %d, training RMSE %.2f, validation RMSE %.2f" % (epochI,
                                                                            Results_train[
                                                                                countSave, 0],
                                                                            Results_val[
                                                                                countSave, 0]))

            if (loss_val[countSave] < lowestInnLoss):
                cnn_model.save(save_path_loss)
                lowestInnLoss = loss_val[countSave]
            countSave += 1

    return loss_train, loss_val, Results_train, Results_val


def get_random_batch(X, y, batch_size):
    '''
    Will return random batches of size batch_size after augmentation

    Params:
        X: numpy array - features
        y: numpy array - classes
        batch_size: Int
    '''
    idx = np.random.randint(0, len(X))

    X_batch = X[idx:idx + batch_size]
    y_batch = y[idx:idx + batch_size]

    # todo Data augmentation, e.g. adding Gaussian noise for generalization purposes
    return X_batch, y_batch


def train_cgan(gan, generator, discriminator,
              X, y, X_val,y_val,
              n_iterations=1000, batch_size=32,
              hist_every=10, log_every=100,
              save_dir='./cgan_model.h5', latent_dim=100):
    '''
    Trains discriminator and generator (last one through the GAN)
    separately in batches of size batch_size. The training goes as follow:
        1. Discriminator is trained with real features from our training data
        2. Discriminator is trained with fake features generated by the Generator
        3. GAN is trained, which will only change the Generator's weights.

    Params:
        gan: GAN model
        generator: Generator model
        discriminator: Discriminator model
        X: numpy array - features
        y: numpy array - classes
        n_iterations: Int
        batch_size: Int
        hist_every: Int - will save the training loss and accuracy every hist_every epochs
        log_every: Int - will output the loss and accuracy every log_every epochs

    Returns:
        loss_real_hist: List of Floats
        acc_real_hist: List of Floats
        loss_fake_hist: List of Floats
        acc_fake_hist: List of Floats
        loss_gan_hist: List of Floats
        acc_gan_hist: List of Floats
    '''

    half_batch = int(batch_size / 2)

    acc_real_hist = []
    acc_fake_hist = []
    acc_gan_hist = []
    loss_real_hist = []
    loss_fake_hist = []
    loss_gan_hist = []
    Results_train = np.zeros((int(np.ceil(n_iterations / hist_every)),5))  # saving the results for each fold in CV, for 160 trials and for 5 metrics
    Results_val = np.zeros((int(np.ceil(n_iterations / hist_every)), 5))  #
    countSave=0
    lowestInnRMSE = 1000
    for iter_i in range(n_iterations):

        X_batch, labels = get_random_batch(X, y, batch_size)

        # train with real values
        y_real = np.ones((X_batch.shape[0], 1))
        loss_real, acc_real = discriminator.train_on_batch([X_batch, labels], y_real)

        # train with fake values
        noise = np.random.uniform(0, 1, (labels.shape[0], latent_dim))
        labels_gen = generator.predict([noise, X_batch])
        y_fake = np.zeros((X_batch.shape[0], 1))
        loss_fake, acc_fake = discriminator.train_on_batch([X_batch, labels_gen], y_fake)

        y_gan = np.ones((labels.shape[0], 1))
        loss_gan, acc_gan = gan.train_on_batch([noise, X_batch], y_gan)

        if (iter_i + 1) % hist_every == 0:
            acc_real_hist.append(acc_real)
            acc_fake_hist.append(acc_fake)
            acc_gan_hist.append(acc_gan)
            loss_real_hist.append(loss_real)
            loss_fake_hist.append(loss_fake)
            loss_gan_hist.append(loss_gan)


            noise = np.random.uniform(0, 1, (X.shape[0], latent_dim))
            predictionsPerRound = generator.predict([noise, X])[:,0]

            Results_train[countSave, 0] = np.sqrt(mean_squared_error(y, predictionsPerRound))
            Results_train[countSave, 1] = mean_absolute_error(y, predictionsPerRound)
            Results_train[countSave, 2] = r2_score(y, predictionsPerRound)
            corrI = stats.pearsonr(y, predictionsPerRound)
            Results_train[countSave, 3] = corrI[0]
            Results_train[countSave, 4] = corrI[1]

            # Results on validation data
            noise = np.random.uniform(0, 1, (X_val.shape[0], latent_dim))
            predictionsPerRound = generator.predict([noise, X_val])[:,0]

            Results_val[countSave, 0] = np.sqrt(mean_squared_error(y_val, predictionsPerRound))
            Results_val[countSave, 1] = mean_absolute_error(y_val, predictionsPerRound)
            Results_val[countSave, 2] = r2_score(y_val, predictionsPerRound)
            corrI = stats.pearsonr(y_val, predictionsPerRound)
            Results_val[countSave, 3] = corrI[0]
            Results_val[countSave, 4] = corrI[1]
            print("Iteration # %d, Training RMSE %.2f, training corr %.2f, validation RMSE %.2f, validation corr %.2f" % (
                    iter_i,
                    Results_train[countSave, 0],
                    Results_train[countSave, 3],
                    Results_val[countSave, 0],
                    Results_val[countSave, 3]))

            # Saving the model with the highest validation correlation

            if (Results_val[countSave, 0] < lowestInnRMSE and Results_val[countSave, 0] ==
                    Results_val[countSave, 0]):
                gan.save(save_dir)
                lowestInnRMSE = Results_val[countSave, 0]
                print("Model saved in file: %s" % save_dir)
            countSave = countSave + 1


            lr = 'loss real: {:.3f}'.format(loss_real)
            ar = 'acc real: {:.3f}'.format(acc_real)
            lf = 'loss fake: {:.3f}'.format(loss_fake)
            af = 'acc fake: {:.3f}'.format(acc_fake)
            lg = 'loss gan: {:.3f}'.format(loss_gan)
            ag = 'acc gan: {:.3f}'.format(acc_gan)

            print('{}, {} | {}, {} | {}, {}'.format(lr, ar, lf, af, lg, ag))

    return loss_real_hist, acc_real_hist, loss_fake_hist, acc_fake_hist, loss_gan_hist, acc_gan_hist,Results_train, Results_val
